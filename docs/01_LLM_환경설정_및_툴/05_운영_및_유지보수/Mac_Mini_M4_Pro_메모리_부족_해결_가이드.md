# 맥 미니 M4 Pro (64GB) 로컬 LLM 메모리 부족 문제 해결 가이드

M4 Pro 칩과 64GB 통합 메모리를 갖춘 맥 미니는 뛰어난 성능을 자랑하지만, 로컬 LLM(대형 언어 모델)을 실행할 때 macOS의 메모리 할당 정책으로 인해 "메모리 부족" 오류를 겪을 수 있습니다. 64GB 램에도 불구하고 이러한 오류가 발생하는 이유와 해결 방법을 정리해 드립니다.

## 1. 왜 64GB인데 부족하다고 뜰까?

가장 큰 이유는 macOS의 GPU 메모리 할당 제한 때문입니다.

*   **기본 제한**: macOS는 시스템 안정성을 위해 전체 통합 메모리의 약 70~80% 정도만 GPU(Metal) 전용으로 할당하도록 기본 설정되어 있습니다.
*   **계산**: 64GB 모델의 경우 실제 GPU가 쓸 수 있는 기본 한도는 약 45GB~48GB 내외입니다.
*   **모델 크기**: `qwen3-coder-next` (70B급 이상 모델 기준)의 파라미터가 70B급 이상이라면, `q4_K_M` 양자화 시 모델 파일 크기만 40GB를 훌쩍 넘습니다. 여기에 **KV Cache(문맥 유지 메모리)**가 추가되면 48GB 제한을 금방 넘기게 됩니다.

## 2. 해결 방법 1: GPU 메모리 할당량 늘리기 (강력 추천)

터미널에서 명령어를 통해 시스템이 GPU에 더 많은 메모리를 빌려줄 수 있도록 강제 설정할 수 있습니다. (재부팅 후에도 유지하려면 설정 파일 수정이 필요하지만, 일단 테스트용으로 실행해 보세요.)

터미널(Terminal)을 열고 아래 명령어를 입력하세요:

```bash
# GPU가 사용할 수 있는 최대 메모리 비율을 90% 이상으로 상향 (비밀번호 입력 필요)
sudo sysctl debug.lowpri_throttle_enabled=0
sudo sysctl iogpu.wired_limit_mb=57344
```

(참고: `57344`는 64GB의 약 89%인 56GB를 의미합니다. 시스템 운영을 위해 최소 4-8GB는 남겨두는 것이 좋습니다.)

### 상세 설명: GPU 메모리 할당량 강제 상향 (`iogpu.wired_limit_mb`)

애플 실리콘(M1, M2, M3, M4)의 통합 메모리 구조는 CPU와 GPU가 램을 공유합니다. 하지만 macOS는 시스템이 멈추는 것을 방지하기 위해 GPU가 전체 램의 약 2/3 이상을 점유하지 못하도록 소프트웨어적으로 잠금을 걸어둡니다.

64GB 램을 장착했어도 실제로 GPU가 쓸 수 있는 양은 기본적으로 약 42~44GB 정도로 제한됩니다. `qwen3-coder-next:q4_K_M` 같은 대형 모델은 모델 자체 크기만 40GB가 넘는 경우가 많아, 로딩 직후 OS가 사용할 여유 공간이 부족해지면 바로 에러를 뱉는 것이죠.

**설정 방법 (터미널)**

터미널을 열고 아래 명령어를 입력하여 GPU의 한계치를 90% 수준으로 끌어올립니다.

```bash
# GPU가 사용할 수 있는 최대 유선 메모리(Wired Memory) 한도를 약 56GB로 설정
sudo sysctl iogpu.wired_limit_mb=57344
```

`57344`의 의미: `64 × 1024 × 0.875` (64GB의 약 87.5%).

**주의사항**: 이 설정은 휘발성입니다. 재부팅하면 다시 기본값으로 돌아오므로, 모델을 돌리기 전에 매번 입력하거나 별도의 스크립트로 등록해두는 것이 좋습니다.

## 3. 해결 방법 2: 컨텍스트 길이(Context Length) 제한

모델이 읽어들일 수 있는 토큰 수(`num_ctx`)가 커질수록 필요한 메모리가 기하급수적으로 늘어납니다.

Ollama를 사용 중이라면 Modelfile을 수정하거나 실행 시 설정을 바꿉니다.

기본값이 32k나 128k로 설정되어 있다면 **8192(8k)** 또는 **4096(4k)**으로 줄여보세요. 이것만으로도 수 GB의 VRAM을 절약할 수 있습니다.

### 상세 설명: 컨텍스트 길이(`num_ctx`) 제한과 KV Cache

LLM을 실행할 때 메모리를 잡아먹는 범인은 크게 두 가지입니다.

*   **모델 가중치(Weights)**: 모델 자체의 용량 (파일 크기).
*   **KV 캐시(Key-Value Cache)**: 대화 내용(컨텍스트)을 기억하기 위한 작업 공간.

문제는 **컨텍스트 길이(`N_ctx`)**가 길어질수록 KV 캐시가 차지하는 메모리가 기하급수적으로 늘어난다는 점입니다. 예를 들어, `qwen3` 같은 최신 모델은 기본 컨텍스트가 128k(약 13만 토큰)까지 지원되는데, 이를 다 활용하려면 가중치 외에 수십 GB의 VRAM이 추가로 필요합니다.

**해결 전략: 필요한 만큼만 쓰기**

코딩 작업을 할 때 파일 전체를 다 밀어 넣는 게 아니라면, 컨텍스트를 **8,192(8k)**나 16,384(16k) 정도로만 제한해도 메모리 사용량을 엄청나게 줄일 수 있습니다.

**Ollama를 사용 중일 때 설정법:**

**터미널에서 직접 실행 시:**

```bash
ollama run qwen3-coder-next:q4_K_M --set parameter num_ctx 8192
```

**Modelfile을 만들어서 고정하기:**

```dockerfile
FROM qwen3-coder-next:q4_K_M
PARAMETER num_ctx 8192
```

이후 `ollama create qwen3-fixed -f Modelfile` 명령어로 나만의 최적화 모델을 생성해 사용하세요.

## 4. 해결 방법 3: 양자화(Quantization) 등급 조절

만약 위 설정 후에도 모델이 로드되지 않는다면, 현재 사용하려는 `q4_K_M`이 M4 Pro의 실질적인 가용 메모리 한계점에 딱 걸쳐 있는 상태일 가능성이 높습니다.

`Q3_K_L` 또는 `Q3_K_M` 버전으로 내려보세요. 4비트와 3비트의 지능 차이는 코딩 작업 시 생각보다 크지 않을 수 있지만, 메모리 점유율은 20% 이상 줄어듭니다.

현재 사양에서는 Qwen 32B 모델 정도가 가장 쾌적하게(매우 빠른 속도로) 돌아갈 최적의 체급입니다.

## 요약 및 제언

1.  **우선순위 1**: 터미널 명령어로 `iogpu.wired_limit_mb`를 상향하세요.
2.  **우선순위 2**: 실행 중인 메모리 점유 앱(크롬 탭 수십 개, Xcode 등)을 종료하여 시스템 여유 메모리를 확보하세요.
3.  **우선순위 3**: `num_ctx`를 낮추어 테스트해 보세요.

**결론적으로 어떻게 되나요?**

위 두 가지 조치(`iogpu.wired_limit_mb` 상향 및 `num_ctx` 제한)를 취하면 다음과 같은 변화가 생깁니다.

*   **기본 상태**: GPU 가용량 43GB < (모델 41GB + 컨텍스트 128k 20GB) → 메모리 부족(OOM) 발생
*   **조치 후**: GPU 가용량 56GB > (모델 41GB + 컨텍스트 8k 4GB) → 여유롭게 실행 가능

현재 사용하시는 환경이 Ollama인가요, 아니면 LM Studio 같은 GUI 툴인가요? 툴에 맞춰서 구체적인 설정 메뉴 위치를 가이드해 드릴 수 있습니다.
