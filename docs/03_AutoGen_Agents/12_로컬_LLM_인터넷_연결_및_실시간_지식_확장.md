# 로컬 LLM 인터넷 연결 및 실시간 지식 확장 가이드

로컬 LLM에게 단순히 "인터넷 권한"만 준다고 해서 스스로 구글링을 하여 똑똑해지는 것은 아닙니다. LLM은 기본적으로 '채팅창에 입력된 텍스트'만 읽을 수 있는 존재이기 때문입니다. RAG(Retrieval-Augmented Generation)를 통해 인터넷 지식을 연결하려면, 모델과 인터넷 사이를 이어주는 **'손과 발(도구)'**이 필요합니다.

---

## 1. 웹 검색 기능이 포함된 UI 사용 (가장 추천)

직접 코딩할 필요 없이, 로컬 LLM을 웹 검색과 연결해 주는 프로그램을 사용하는 방식입니다.

*   **Open WebUI**: Ollama와 찰떡궁합인 UI입니다. 설정에서 'Web Search' 기능을 켜고 **SearXNG** 같은 검색 엔진을 연결하면, 질문했을 때 모델이 실시간으로 인터넷을 검색한 뒤 그 내용을 읽고 답변합니다.
*   **SearXNG란?**: 구글이나 네이버가 사람이 쓰는 검색 사이트라면, SearXNG는 AI가 검색 결과를 가져가기 좋게 여러 검색 엔진(구글, 빙, 덕덕고 등)의 결과를 한데 모아서 전달해주는 **'중계 검색 엔진'**입니다.
*   **간편 연결**: 직접 구축이 번거롭다면 **Tavily**나 **Serper** 같은 서비스의 API 키를 발급받아 넣는 것이 가장 빠릅니다.

### 🔍 검색 방식 비교: API vs 직접 설치 (SearXNG)

| 비교 항목 | API 방식 (Tavily, Serper) | 직접 설치 (SearXNG) |
| :--- | :--- | :--- |
| **비용** | 기본 무료 (사용량 초과 시 유료) | **완전 무료** |
| **설치 난이도** | **매우 쉬움** (키 복사만 하면 끝) | 보통~어려움 (Docker 설정 필요) |
| **검색 품질** | AI 최적화 (깔끔한 본문 추출) | 일반 검색 결과 (직접 정제 필요) |
| **개인정보** | 외부 서버를 거침 | 내 로컬 환경 내에서 해결 |

---

## 2. 주요 검색 엔진 상세 및 요금 체계

### 1) Tavily (AI 검색 최적화)
AI가 읽기 좋게 웹사이트 본문만 긁어다 주기 때문에 RAG 효율이 가장 좋습니다.
*   **무료**: 월 1,000건 (하루 약 33회)
*   **유료**: Starter($12/월, 5k건), Pro($24/월, 10k건)
*   **특징**: Qwen 모델이 읽어야 할 텍스트 양을 줄여줘서 답변 속도가 매우 빠릅니다.

### 2) Serper.dev (가성비 구글 검색)
실제 구글 검색 결과를 AI가 사용하기 좋게 전달해주는 서비스입니다.
*   **무료**: 가입 시 **2,500건 무료 크레딧** 제공 (평생 1회)
*   **유료**: $1당 1,000건 (충전식 가능, 매우 저렴)
*   **특징**: 진짜 '구글 검색 결과' 그대로를 가져오고 싶을 때 최고의 선택입니다.

### 3) SearXNG (로컬 설치형)
내 맥북(M4 Pro)에 직접 구글 검색 중계 서버를 차리는 방식입니다.
*   **비용**: 0원 (완전 무료)
*   **주의**: 구글 등에서 캡차(사람 확인)가 뜰 수 있어 여러 엔진(Bing, DuckDuckGo 등)을 섞어서 관리해야 합니다.

---

## 3. 'Search Tool' 권한 부여 (Function Calling)

모델에게 "모르는 정보가 나오면 이 검색 도구를 사용해라"라고 명시적으로 권한을 주는 방식입니다.

### 💡 Function Calling 작동 원리 (모델에게 도구 쥐여주기)
모델이 스스로 도구를 찾아내는 것이 아니라, 우리가 모델에게 **'도구 설명서(카탈로그)'**를 미리 읽어주는 과정입니다.

1.  **도구 카탈로그 전달**: 대화 시작 전, 모델에게 JSON 형식의 설명서를 보냅니다. "너는 `search_google`이라는 도구를 쓸 수 있어. 모르는 게 나오면 직접 답하지 말고 이 도구를 호출해줘."
2.  **모델의 판단 (The "Aha!" Moment)**: 사용자가 "오늘 삼성전자 주가 알려줘"라고 물으면, 모델은 자신의 지식에 정보가 없음을 인지하고 설명서의 `search_google`을 떠올립니다.
3.  **도구 호출**: 모델이 "나 `search_google` 쓸래. 검색어는 '삼성전자 주가'야"라고 외칩니다.
4.  **중계자(System)의 역할**: Ollama나 Open WebUI 같은 중계자가 이 요청을 받아 실제로 검색을 수행하고, 결과 텍스트를 다시 모델에게 전달합니다.
5.  **최종 답변**: 모델은 전달받은 검색 결과를 바탕으로 "현재 삼성전자 주가는 75,000원입니다"라고 답변합니다.

**Qwen 2.5** 시리즈는 이러한 약속된 형식을 이해하고 따르는 능력이 매우 뛰어나 Function Calling에 매우 적합합니다.

### LiteLLM으로 OpenAI 클라이언트에 검색 도구 붙이기

Open WebUI 대신 **Antigravity**, **Continue** 같은 OpenAI 규격 클라이언트를 쓰는 경우, **LiteLLM**을 중간에 두면 Ollama + Serper(검색)를 한 엔드포인트로 제공할 수 있습니다. LiteLLM이 "인터넷 검색이 필요해"라는 모델 신호를 받아 Serper API를 대신 호출하고 결과를 다시 모델에게 전달합니다. 상세 구성은 [01_Setup 08. LiteLLM – OpenAI 호환 프록시](../01_Setup/08_LiteLLM_OpenAI_호환_프록시.md)를 참고하세요.

## 4. RAG와 실시간 웹 검색의 차이

로컬 LLM에게 인터넷 권한을 준다는 것은 "모델에게 브라우저를 쓸 수 있는 능력을 부여하는 것"과 같습니다.

| 구분 | 일반 RAG | 웹 검색 RAG (실시간 연결) |
| :--- | :--- | :--- |
| **데이터 소스** | 내가 가진 로컬 문서 (PDF, 텍스트) | 전 세계 인터넷 웹사이트 |
| **방식** | 문서를 DB(Vector)에 넣어두고 검색 | 질문 시 실시간으로 인터넷 검색 |
| **활용 목적** | 기업 내부 기밀, 특정 지식 학습 | 최신 뉴스, 날씨, 실시간 정보 확인 |

---

## 💡 추천 실행 전략: Serper.dev 기반 지능 극대화

1.  **가장 빠른 연결**: [Serper.dev](https://serper.dev)에 가입하여 무료 2,500건 크레딧을 확보하세요. 카드 등록 없이 구글 로그인만으로 즉시 키 발급이 가능합니다.
2.  **Open WebUI 연동**: `Settings` -> `Web Search`에서 `Serper`를 선택하고 키를 입력합니다.
3.  **Router(중계자) 전담**: `Qwen 2.5 7B` 같은 가벼운 모델에게 검색 도구(Serper)를 전담시키세요.
    *   **흐름**: 사용자 질문 -> Router가 구글 검색 후 요약 -> Planner/Coder에게 전달.
    *   이 방식을 통해 LLM의 '과거 지식 한계'를 M4 Pro 맥북 내부에서 완전히 극복할 수 있습니다.
4.  **장기적 관점**: 사용량이 많아지거나 검색 기록 보호가 중요해지면 그때 `SearXNG` 로컬 설치를 고려하세요.

---

## 5. 근거 오염(Grounding Pollution) 방지 및 비판적 사고 전략

인터넷 검색 결과가 반드시 '진리'는 아닙니다. AI가 인터넷의 가짜 뉴스나 편향된 정보를 맹목적으로 믿는 현상을 **'근거 오염(Grounding Pollution)'**이라고 합니다. 이를 방지하기 위한 3단계 방어 전략을 제안합니다.

### 1) Modelfile에 '비판적 사고' 주입
중계자(Router)나 플래너의 SYSTEM 프롬프트에 인터넷 정보는 참고용일 뿐이라는 가이드라인을 강력하게 설정해야 합니다.

```dockerfile
# Modelfile SYSTEM 프롬프트 예시
SYSTEM """
너는 고도로 숙련된 설계자다. 인터넷 검색 결과(Serper)를 접할 때 아래 원칙을 반드시 지켜라:
1. 검색 결과에 상충하는 내용이 있다면 반드시 사용자에게 보고하고 양쪽 입장을 모두 설명하라.
2. 특정 블로그나 개인의 의견보다는 공식 문서(Official Docs)와 신뢰도 높은 뉴스 소스를 우선하라.
3. 검색된 내용이 기존 지식(너의 파라미터)과 심각하게 충돌할 경우, '확인이 필요한 정보'라고 명시하라.
4. 절대 인터넷의 내용을 맹목적으로 '진리'라고 단정 짓지 마라.
"""
```

### 2) 교차 검증(Cross-Verification) 프로세스
에이전시 구성 중 **👀 Dev2_Reviewer**의 역할을 강화하여 상호 견제 시스템을 구축합니다.

*   **프로세스**: Router 검색 -> Dev1(Coder) 작업 -> **Dev2(Reviewer)**가 최신 보안 라이브러리 기준이나 공식 가이드에 맞는지 재검토.
*   **효과**: 한 모델이 낚이더라도, 다른 페르소나를 가진 모델이 이를 걸러내는 '상호 견제'가 일어납니다.

### 3) Open WebUI의 'RAG 설정' 최적화
검색 결과 중 상위 일부만 참고하도록 설정하여 정보의 질을 관리합니다.

*   **Top K 값 조절**: 검색 결과 중 상위 3~5개만 보게 하여 너무 잡다한 정보가 섞이지 않게 합니다.
*   **Score Threshold**: 유사도 점수가 낮은(관련성 떨어지는) 정보는 아예 읽지 못하게 차단합니다.

---

## 💡 최종 조언

인터넷 연결은 모델에게 '눈'을 달아주는 것이지, '판단력' 자체를 대신해주지는 않습니다. 결국 최종 결정권자인 **👤 Human_Lead**가 모델이 답변 하단에 달아준 [Serper 소스 링크]를 가끔 클릭해 보며 정보의 신뢰도를 체크하는 프로세스가 병행될 때 가장 완벽한 에이전시가 됩니다.
"모델의 답변에 항상 의문을 제기하라"는 지침을 시스템 프롬프트에 추가하는 것만으로도 모델은 훨씬 신중해집니다!

*   **모델의 성능**: 너무 작은 모델(예: 1B, 3B)은 인터넷에서 가져온 방대한 정보를 요약하거나 설명서를 이해하는 능력이 부족할 수 있습니다. **최소 7B~8B 이상(Qwen 2.5 7B 권장)**의 모델 사용을 추천합니다.
*   **할루시네이션(환각)**: 인터넷에서 잘못된 정보를 가져올 경우, 모델이 이를 '진리'라고 믿고 당당하게 오답을 낼 수 있습니다. 검색 결과의 신뢰도 관리가 중요합니다.

---

## 6. RAG 및 웹 검색 최종 체크리스트

*   [ ] **검색 API 연동**: Tavily 또는 Serper API 키 발급 및 Open WebUI 등록 완료
*   [ ] **임베딩 모델 설정**: `multi-qa-mpnet-base-dot-v1` 또는 `bge-m3` 모델 다운로드 및 지정
*   [ ] **컨텍스트 윈도우 확장**: Modelfile에서 `num_ctx`를 32768(32k) 이상으로 설정
*   [ ] **Top K / Score Threshold**: Open WebUI RAG 설정에서 검색 결과 수와 신뢰도 점수 최적화
*   [ ] **비판적 사고 시스템 프롬프트**: Router 및 Architect 모델에 '공식 문서 우선' 및 '데이터 매칭 검증' 로직 주입 완료
