# 로컬 LLM 인터넷 연결 및 실시간 지식 확장 가이드

로컬 LLM에게 단순히 "인터넷 권한"만 준다고 해서 스스로 구글링을 하여 똑똑해지는 것은 아닙니다. LLM은 기본적으로 '채팅창에 입력된 텍스트'만 읽을 수 있는 존재이기 때문입니다. RAG(Retrieval-Augmented Generation)를 통해 인터넷 지식을 연결하려면, 모델과 인터넷 사이를 이어주는 **'손과 발(도구)'**이 필요합니다.

---

## 0. 핵심 원리: 로컬 LLM은 직접 검색할 수 없다

**로컬 LLM(모델 그 자체)은 "인터넷 연결 스위치가 없는 뇌"**와 같습니다. ChatGPT·Claude는 서비스 운영사가 붙여준 **'검색 엔진 전용 비서'** 덕분에 검색이 가능한 것이고, 로컬 LLM 환경에서는 그 비서 역할을 직접 세팅해줘야 합니다.

### 로컬 LLM의 한계

| 구분 | 설명 |
| :--- | :--- |
| **폐쇄형 구조** | 모델은 파일 덩어리일 뿐이다. 스스로 브라우저를 띄워 구글링할 능력이 없다. |
| **환각(Hallucination)** | 최신 정보를 물으면 모른다고 하거나, 학습된 데이터 중 비슷한 내용을 지어낸다. |

### LiteLLM + Serper가 해결하는 방식 (검색 비서 추가)

로컬 LLM에 **'도구(Tool)'**라는 손발을 달아주는 구조입니다.

| 단계 | 주체 | 행동 |
| :--- | :--- | :--- |
| 1. 질문 | 안티그래비티(클라이언트) | "오늘 삼성전자 주가 알려줘." |
| 2. 판단 | 로컬 LLM | "내 뇌에는 오늘 주가 데이터가 없네? search 도구를 써야겠다." (신호 발생) |
| 3. 실행 | LiteLLM | LLM의 신호를 가로채서 Serper API에 접속, 실제 구글 검색 결과를 가져옴. |
| 4. 전달 | LiteLLM | 검색된 텍스트를 로컬 LLM에게 다시 전달. |
| 5. 추론 | 로컬 LLM | 전달받은 최신 정보를 바탕으로 문장을 다듬어 답변 생성. |

**왜 이 구조가 핵심인가?**

- **안티그래비티는 상대가 GPT-4인지, 맥미니의 Llama인지 모른다.** 답변만 잘 오면 된다.
- **LiteLLM**이 로컬 LLM이 검색을 수행할 수 있도록 중간에서 **에이전트(Agent)** 역할을 대행한다.
- 결과적으로 **맥미니의 성능(M4 64GB)**을 쓰면서도, 유료 결제 모델처럼 **실시간 검색이 가미된 답변**을 받을 수 있다.

**정리:** *"로컬 LLM은 직접 검색을 못 한다"*는 한계를 **LiteLLM이라는 중계자**로 돌파하는 것이 이번 구조의 본질이다. 결국 **"LLM(뇌) + 검색 API(손발) + 중계자(비서)"**라는 3단 구조가 동일하다.

### 활용 가치를 높여줄 도구

| 도구 이름 | 설명 |
| :--- | :--- |
| **Tailscale** | 맥미니와 iPhone을 가상 사설망으로 묶어 준다. 외부에서도 `http://맥미니-IP:4000` 주소만으로 검색 가능한 로컬 LLM 서버에 안전하게 접속하는 '보안 터널'. |
| **Open WebUI** | 단순 채팅창을 넘어, 검색 결과나 문서(PDF)를 시각적으로 확인하며 로컬 LLM과 대화할 수 있는 웹 인터페이스. 맥미니에 띄워두면 편리하다. |

---

## 1. 웹 검색 기능이 포함된 UI 사용 (가장 추천)

직접 코딩할 필요 없이, 로컬 LLM을 웹 검색과 연결해 주는 프로그램을 사용하는 방식입니다.

*   **Open WebUI**: Ollama와 찰떡궁합인 UI입니다. 설정에서 'Web Search' 기능을 켜고 **SearXNG** 같은 검색 엔진을 연결하면, 질문했을 때 모델이 실시간으로 인터넷을 검색한 뒤 그 내용을 읽고 답변합니다.
*   **SearXNG란?**: 구글이나 네이버가 사람이 쓰는 검색 사이트라면, SearXNG는 AI가 검색 결과를 가져가기 좋게 여러 검색 엔진(구글, 빙, 덕덕고 등)의 결과를 한데 모아서 전달해주는 **'중계 검색 엔진'**입니다.
*   **간편 연결**: 직접 구축이 번거롭다면 **Tavily**나 **Serper** 같은 서비스의 API 키를 발급받아 넣는 것이 가장 빠릅니다.

### 🔍 검색 방식 비교: API vs 직접 설치 (SearXNG)

| 비교 항목 | API 방식 (Tavily, Serper) | 직접 설치 (SearXNG) |
| :--- | :--- | :--- |
| **비용** | 기본 무료 (사용량 초과 시 유료) | **완전 무료** |
| **설치 난이도** | **매우 쉬움** (키 복사만 하면 끝) | 보통~어려움 (Docker 설정 필요) |
| **검색 품질** | AI 최적화 (깔끔한 본문 추출) | 일반 검색 결과 (직접 정제 필요) |
| **개인정보** | 외부 서버를 거침 | 내 로컬 환경 내에서 해결 |

### Open WebUI vs LiteLLM 검색 구조 비교

Open WebUI의 검색 기능도 구조적으로는 **LiteLLM + Serper**와 같은 원리로 작동한다. 차이는 **'누가 그 역할을 맡느냐'**이다.

| 구분 | Open WebUI (내장형 에이전트) | LiteLLM (독립형 엔드포인트) |
| :--- | :--- | :--- |
| **특징** | 자체 설정 메뉴에 Web Search 옵션이 있고, Serper·Google Search API 키를 넣을 수 있다. | 화면이 없는 '중계 서버'. 모델에 검색 기능을 '박아넣은' 것처럼 속여서 외부로 내보낸다. |
| **흐름** | 사용자 질문 → Open WebUI가 판단 → 직접 Serper 호출 → 검색 결과를 LLM(Ollama)에게 전달 → 답변 생성. | 사용자 질문 → LiteLLM이 판단/실행 → Serper 호출 → 결과를 LLM에게 전달 → 답변 생성. |
| **장점** | 설정이 매우 쉽고 UI가 예쁘다. | **안티그래비티**, iPhone 앱 등 Open WebUI가 아닌 외부 앱에서도 똑같이 검색 기능을 쓸 수 있다. |

**어떤 것을 선택할까?**

| 선택지 | 추천 상황 |
| :--- | :--- |
| **Open WebUI** | 주로 웹 브라우저로 맥미니에 접속해 채팅하고 싶을 때. (가장 간편) |
| **LiteLLM** | 안티그래비티(IDE) 안에서 코딩하며 검색 기능을 쓰거나, 다양한 서드파티 앱을 연동하고 싶을 때. |

핵심 니즈가 **"안티그래비티 내에서의 활용"**이면, Open WebUI보다 **LiteLLM**으로 '검색 능력이 탑재된 가상 모델'을 하나 만들어두는 것이 범용성이 높다.

---

## 2. 주요 검색 엔진 상세 및 요금 체계

### 1) Tavily (AI 검색 최적화)
AI가 읽기 좋게 웹사이트 본문만 긁어다 주기 때문에 RAG 효율이 가장 좋습니다.
*   **무료**: 월 1,000건 (하루 약 33회)
*   **유료**: Starter($12/월, 5k건), Pro($24/월, 10k건)
*   **특징**: Qwen 모델이 읽어야 할 텍스트 양을 줄여줘서 답변 속도가 매우 빠릅니다.

### 2) Serper.dev (가성비 구글 검색)
실제 구글 검색 결과를 AI가 사용하기 좋게 전달해주는 서비스입니다.
*   **무료**: 가입 시 **2,500건 무료 크레딧** 제공 (평생 1회)
*   **유료**: $1당 1,000건 (충전식 가능, 매우 저렴)
*   **특징**: 진짜 '구글 검색 결과' 그대로를 가져오고 싶을 때 최고의 선택입니다.

### 3) SearXNG (로컬 설치형)
내 맥북(M4 Pro)에 직접 구글 검색 중계 서버를 차리는 방식입니다.
*   **비용**: 0원 (완전 무료)
*   **주의**: 구글 등에서 캡차(사람 확인)가 뜰 수 있어 여러 엔진(Bing, DuckDuckGo 등)을 섞어서 관리해야 합니다.
*   **활용**: Serper 같은 유료 API 대신, 맥미니에 직접 설치해 쓸 수 있는 오픈소스 검색 엔진. Open WebUI나 LiteLLM에 연결하면 **자급자족형** 검색 환경을 만들 수 있다.

### 4) Brave Search API
Serper 외에 개발자들이 선호하는 검색 API입니다.
*   **특징**: 광고 없는 깨끗한 검색 데이터를 LLM에게 전달하기에 최적화되어 있다.
*   **연동**: Open WebUI 또는 LiteLLM 설정에서 검색 백엔드로 지정 가능.

---

## 3. 'Search Tool' 권한 부여 (Function Calling)

모델에게 "모르는 정보가 나오면 이 검색 도구를 사용해라"라고 명시적으로 권한을 주는 방식입니다.

### 💡 Function Calling 작동 원리 (모델에게 도구 쥐여주기)
모델이 스스로 도구를 찾아내는 것이 아니라, 우리가 모델에게 **'도구 설명서(카탈로그)'**를 미리 읽어주는 과정입니다.

1.  **도구 카탈로그 전달**: 대화 시작 전, 모델에게 JSON 형식의 설명서를 보냅니다. "너는 `search_google`이라는 도구를 쓸 수 있어. 모르는 게 나오면 직접 답하지 말고 이 도구를 호출해줘."
2.  **모델의 판단 (The "Aha!" Moment)**: 사용자가 "오늘 삼성전자 주가 알려줘"라고 물으면, 모델은 자신의 지식에 정보가 없음을 인지하고 설명서의 `search_google`을 떠올립니다.
3.  **도구 호출**: 모델이 "나 `search_google` 쓸래. 검색어는 '삼성전자 주가'야"라고 외칩니다.
4.  **중계자(System)의 역할**: Ollama나 Open WebUI 같은 중계자가 이 요청을 받아 실제로 검색을 수행하고, 결과 텍스트를 다시 모델에게 전달합니다.
5.  **최종 답변**: 모델은 전달받은 검색 결과를 바탕으로 "현재 삼성전자 주가는 75,000원입니다"라고 답변합니다.

**Qwen 2.5** 시리즈는 이러한 약속된 형식을 이해하고 따르는 능력이 매우 뛰어나 Function Calling에 매우 적합합니다.

### LiteLLM으로 OpenAI 클라이언트에 검색 도구 붙이기

Open WebUI 대신 **Antigravity**, **Continue** 같은 OpenAI 규격 클라이언트를 쓰는 경우, **LiteLLM**을 중간에 두면 Ollama + Serper(검색)를 한 엔드포인트로 제공할 수 있습니다. LiteLLM이 "인터넷 검색이 필요해"라는 모델 신호를 받아 Serper API를 대신 호출하고 결과를 다시 모델에게 전달합니다.

*   **구축 가이드**: 맥미니 M4 64GB 환경에서 LiteLLM + Serper + 안티그래비티를 단계별로 설정하려면 [13. LiteLLM Serper 안티그래비티 구축 가이드](./13_LiteLLM_Serper_안티그래비티_구축_가이드.md)를 참고하세요.
*   **상세 구성**: [01_Setup 08. LiteLLM – OpenAI 호환 프록시](../01_Setup/08_LiteLLM_OpenAI_호환_프록시.md)를 참고하세요.

## 4. RAG와 실시간 웹 검색의 차이

로컬 LLM에게 인터넷 권한을 준다는 것은 "모델에게 브라우저를 쓸 수 있는 능력을 부여하는 것"과 같습니다.

| 구분 | 일반 RAG | 웹 검색 RAG (실시간 연결) |
| :--- | :--- | :--- |
| **데이터 소스** | 내가 가진 로컬 문서 (PDF, 텍스트) | 전 세계 인터넷 웹사이트 |
| **방식** | 문서를 DB(Vector)에 넣어두고 검색 | 질문 시 실시간으로 인터넷 검색 |
| **활용 목적** | 기업 내부 기밀, 특정 지식 학습 | 최신 뉴스, 날씨, 실시간 정보 확인 |

---

## 💡 추천 실행 전략: Serper.dev 기반 지능 극대화

1.  **가장 빠른 연결**: [Serper.dev](https://serper.dev)에 가입하여 무료 2,500건 크레딧을 확보하세요. 카드 등록 없이 구글 로그인만으로 즉시 키 발급이 가능합니다.
2.  **Open WebUI 연동**: `Settings` -> `Web Search`에서 `Serper`를 선택하고 키를 입력합니다.
3.  **Router(중계자) 전담**: `Qwen 2.5 7B` 같은 가벼운 모델에게 검색 도구(Serper)를 전담시키세요.
    *   **흐름**: 사용자 질문 -> Router가 구글 검색 후 요약 -> Planner/Coder에게 전달.
    *   이 방식을 통해 LLM의 '과거 지식 한계'를 M4 Pro 맥북 내부에서 완전히 극복할 수 있습니다.
4.  **장기적 관점**: 사용량이 많아지거나 검색 기록 보호가 중요해지면 그때 `SearXNG` 로컬 설치를 고려하세요.

---

## 5. 근거 오염(Grounding Pollution) 방지 및 비판적 사고 전략

인터넷 검색 결과가 반드시 '진리'는 아닙니다. AI가 인터넷의 가짜 뉴스나 편향된 정보를 맹목적으로 믿는 현상을 **'근거 오염(Grounding Pollution)'**이라고 합니다. 이를 방지하기 위한 3단계 방어 전략을 제안합니다.

### 1) Modelfile에 '비판적 사고' 주입
중계자(Router)나 플래너의 SYSTEM 프롬프트에 인터넷 정보는 참고용일 뿐이라는 가이드라인을 강력하게 설정해야 합니다.

```dockerfile
# Modelfile SYSTEM 프롬프트 예시
SYSTEM """
너는 고도로 숙련된 설계자다. 인터넷 검색 결과(Serper)를 접할 때 아래 원칙을 반드시 지켜라:
1. 검색 결과에 상충하는 내용이 있다면 반드시 사용자에게 보고하고 양쪽 입장을 모두 설명하라.
2. 특정 블로그나 개인의 의견보다는 공식 문서(Official Docs)와 신뢰도 높은 뉴스 소스를 우선하라.
3. 검색된 내용이 기존 지식(너의 파라미터)과 심각하게 충돌할 경우, '확인이 필요한 정보'라고 명시하라.
4. 절대 인터넷의 내용을 맹목적으로 '진리'라고 단정 짓지 마라.
"""
```

### 2) 교차 검증(Cross-Verification) 프로세스
에이전시 구성 중 **👀 Dev2_Reviewer**의 역할을 강화하여 상호 견제 시스템을 구축합니다.

*   **프로세스**: Router 검색 -> Dev1(Coder) 작업 -> **Dev2(Reviewer)**가 최신 보안 라이브러리 기준이나 공식 가이드에 맞는지 재검토.
*   **효과**: 한 모델이 낚이더라도, 다른 페르소나를 가진 모델이 이를 걸러내는 '상호 견제'가 일어납니다.

### 3) Open WebUI의 'RAG 설정' 최적화
검색 결과 중 상위 일부만 참고하도록 설정하여 정보의 질을 관리합니다.

*   **Top K 값 조절**: 검색 결과 중 상위 3~5개만 보게 하여 너무 잡다한 정보가 섞이지 않게 합니다.
*   **Score Threshold**: 유사도 점수가 낮은(관련성 떨어지는) 정보는 아예 읽지 못하게 차단합니다.

---

## 💡 최종 조언

인터넷 연결은 모델에게 '눈'을 달아주는 것이지, '판단력' 자체를 대신해주지는 않습니다. 결국 최종 결정권자인 **👤 Human_Lead**가 모델이 답변 하단에 달아준 [Serper 소스 링크]를 가끔 클릭해 보며 정보의 신뢰도를 체크하는 프로세스가 병행될 때 가장 완벽한 에이전시가 됩니다.
"모델의 답변에 항상 의문을 제기하라"는 지침을 시스템 프롬프트에 추가하는 것만으로도 모델은 훨씬 신중해집니다!

*   **모델의 성능**: 너무 작은 모델(예: 1B, 3B)은 인터넷에서 가져온 방대한 정보를 요약하거나 설명서를 이해하는 능력이 부족할 수 있습니다. **최소 7B~8B 이상(Qwen 2.5 7B 권장)**의 모델 사용을 추천합니다.
*   **할루시네이션(환각)**: 인터넷에서 잘못된 정보를 가져올 경우, 모델이 이를 '진리'라고 믿고 당당하게 오답을 낼 수 있습니다. 검색 결과의 신뢰도 관리가 중요합니다.

---

## 6. RAG 및 웹 검색 최종 체크리스트

*   [ ] **검색 API 연동**: Tavily 또는 Serper API 키 발급 및 Open WebUI 등록 완료
*   [ ] **임베딩 모델 설정**: `multi-qa-mpnet-base-dot-v1` 또는 `bge-m3` 모델 다운로드 및 지정
*   [ ] **컨텍스트 윈도우 확장**: Modelfile에서 `num_ctx`를 32768(32k) 이상으로 설정
*   [ ] **Top K / Score Threshold**: Open WebUI RAG 설정에서 검색 결과 수와 신뢰도 점수 최적화
*   [ ] **비판적 사고 시스템 프롬프트**: Router 및 Architect 모델에 '공식 문서 우선' 및 '데이터 매칭 검증' 로직 주입 완료
