# 11. Ollama Modelfile 최적화 가이드

Ollama Modelfile은 단순한 양자화 설정 도구가 아니라, 모델의 '기본 인격과 설정값'을 구워내는(Baking) 설계도입니다. 시스템의 성능을 극대화하고 API 호출을 효율화하기 위한 Modelfile 활용 전략을 설명합니다.

## 1. Ollama Modelfile vs API System Parameter

| 비교 항목 | Modelfile (Pre-baked) | API System Parameter |
| :--- | :--- | :--- |
| **초기 로딩 속도** | 미세하게 빠름 (KV 캐시 최적화) | 매번 파싱 필요 |
| **전송 오버헤드** | 낮음 (모델 이름만 호출) | 높음 (매번 긴 프롬프트 전송) |
| **관리 편의성** | 모델이 늘어나서 복잡해질 수 있음 | 코드 한 줄로 수정 가능 (유연함) |
| **적합한 용도** | 변하지 않는 전문성, 하드웨어 설정 | 구체적인 작업 지시, 상황별 피드백 |

## 2. Modelfile에 프롬프트를 구워넣는 이유 (장점)

1.  **KV 캐시 최적화 (성능)**: 시스템 프롬프트가 모델의 일부로 고정되면, Ollama가 해당 프롬프트를 처리하는 초기 계산 과정을 최적화할 수 있어 첫 토큰 생성 속도(TTFT)가 미세하게 빨라집니다.
2.  **API 호출 단순화**: 매번 긴 프롬프트를 네트워크로 전달할 필요 없이, 모델 이름만 호출하면 됩니다.
3.  **파라미터 고정**: `temperature`, `num_ctx`, `top_p` 같은 설정을 모델별로 고정할 수 있어, 에이전트별 특성(코더는 낮게, 기획자는 높게)을 유지하기 쉽습니다.

## 3. 가장 효율적인 '하이브리드' 전략 (추천)

M4 Pro 64GB 환경에서 에이전시를 운영하신다면 아래와 같은 단계별 적용을 추천합니다.

### 1단계: Modelfile로 "기본 골격" 고정 (에이전트 생성)
각 에이전트의 변하지 않는 '근본 페르소나'와 '성능 설정'을 Modelfile로 구워버립니다.

```dockerfile
# 예: my-coder-32b 생성용 Modelfile
FROM qwen3-coder:32b

# 코더용 낮은 창의성 및 컨텍스트 창 고정
PARAMETER temperature 0.2

# 컨텍스트 윈도우(Context Window) 설정
# M4 Pro 환경이므로 32768(32k) 정도로 넉넉히 잡으셔도 메모리가 충분합니다.
# Qwen 2.5(3)는 최대 128k까지 지원하지만, 32k가 성능과 메모리 사이의 가성비가 가장 좋습니다.
PARAMETER num_ctx 32768

# 기본 원칙 및 비판적 사고 주입
SYSTEM """
너는 Python과 Clean Code에 정통한 시니어 개발자다.
인터넷 검색 결과를 접할 때 공식 문서를 최우선하고, 맹목적으로 진리라 단정 짓지 마라.
답변은 반드시 JSON으로만 한다.
"""
```

### 2단계: API 호출 시 "현재 상황" 주입
Modelfile로 정의된 모델을 호출할 때, `system` 파라미터에는 '지금 당장 해야 할 구체적인 업무'만 짧게 넘깁니다.

**API 호출 예시:**
```json
{
  "model": "my-coder-32b",
  "system": "현재 'login.py'의 42번 라인 에러를 수정하라."
}
```

이렇게 하면 모델은 **Modelfile의 기본 인격 + API의 구체적 지시**를 합쳐서 이해합니다.

## 💡 결론 및 가이드

1.  **개발 및 테스트 단계**: API 호출 시 `system` 파라미터로 넘기는 방식으로 프롬프트를 계속 깎으면서 테스트하세요. 수정과 확인이 가장 빠릅니다.
2.  **운영 및 안정화 단계**: 시스템이 안정화되고 "이 프롬프트는 이제 절대 안 바뀐다" 싶을 때, 해당 내용을 Modelfile에 넣어 나만의 전용 에이전트 모델(예: `qwen3-planner-v1`, `qwen3-tester-v1`)을 만드시는 것이 성능과 관리 측면에서 가장 유리합니다.
