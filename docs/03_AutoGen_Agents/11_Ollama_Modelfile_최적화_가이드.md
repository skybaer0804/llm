# 11. Ollama Modelfile 최적화 가이드

Ollama Modelfile은 단순한 양자화 설정 도구가 아니라, 모델의 '기본 인격과 설정값'을 구워내는(Baking) 설계도입니다. 시스템의 성능을 극대화하고 API 호출을 효율화하기 위한 Modelfile 활용 전략을 설명합니다.

## 1. Ollama Modelfile vs API System Parameter

| 비교 항목 | Modelfile (Pre-baked) | API System Parameter |
| :--- | :--- | :--- |
| **초기 로딩 속도** | 미세하게 빠름 (KV 캐시 최적화) | 매번 파싱 필요 |
| **전송 오버헤드** | 낮음 (모델 이름만 호출) | 높음 (매번 긴 프롬프트 전송) |
| **관리 편의성** | 모델이 늘어나서 복잡해질 수 있음 | 코드 한 줄로 수정 가능 (유연함) |
| **적합한 용도** | 변하지 않는 전문성, 하드웨어 설정 | 구체적인 작업 지시, 상황별 피드백 |

## 2. Modelfile에 프롬프트를 구워넣는 이유 (장점)

1.  **KV 캐시 최적화 (성능)**: 시스템 프롬프트가 모델의 일부로 고정되면, Ollama가 해당 프롬프트를 처리하는 초기 계산 과정을 최적화할 수 있어 첫 토큰 생성 속도(TTFT)가 미세하게 빨라집니다.
2.  **API 호출 단순화**: 매번 긴 프롬프트를 네트워크로 전달할 필요 없이, 모델 이름만 호출하면 됩니다.
3.  **파라미터 고정**: `temperature`, `num_ctx`, `top_p` 같은 설정을 모델별로 고정할 수 있어, 에이전트별 특성(코더는 낮게, 기획자는 높게)을 유지하기 쉽습니다.

## 3. 가장 효율적인 '하이브리드' 전략 (추천)

M4 Pro 64GB 환경에서 에이전시를 운영하신다면 아래와 같은 단계별 적용을 추천합니다.

### 1단계: Modelfile로 "기본 골격" 고정 (에이전트 생성)
각 에이전트의 변하지 않는 '근본 페르소나'와 '성능 설정'을 Modelfile로 구워버립니다.

```dockerfile
# 예: my-coder-32b 생성용 Modelfile
FROM qwen3-coder:30b

# 코더용 낮은 창의성 및 컨텍스트 창 고정
PARAMETER temperature 0.2

# 컨텍스트 윈도우(Context Window) 설정
# M4 Pro 환경이므로 32768(32k) 정도로 넉넉히 잡으셔도 메모리가 충분합니다.
# Qwen 2.5(3)는 최대 128k까지 지원하지만, 32k가 성능과 메모리 사이의 가성비가 가장 좋습니다.
PARAMETER num_ctx 32768

---

## 4. 컨텍스트 크기(num_ctx) 확인 및 고정 가이드

맥미니(56GB/64GB) 환경에서 Ollama의 기본 컨텍스트(2048)는 프로젝트 전체를 파악하기에 너무 작습니다. 32k(32768)로 확장하여 '기억력'을 극대화하는 것이 성능의 핵심입니다.

### 4.1 현재 컨텍스트 크기 확인법 (How to Check)

#### 방법 1: 서버 로그 확인 (가장 확실함)
맥미니 터미널에서 아래 명령어로 실시간 로그를 확인합니다.
```bash
journalctl -u ollama --no-pager | grep num_ctx
```
- `llama_new_context_with_model: n_ctx = 32768` 또는 `kv_self.n_ctx = 32768` 문구가 현재 설정값입니다.

#### 방법 2: API를 통한 확인 (외부 노트북에서도 가능)
노트북 터미널에서 맥미니 IP로 요청을 보냅니다.
```bash
curl http://[맥미니IP]:11434/api/show -d '{
  "name": "qwen3-coder-next:q4_K_M"
}'
```
- 결과값 중 `parameters` 섹션이나 모델 정의 부분에 `num_ctx` 값이 명시되어 있는지 확인하세요. 만약 없다면 기본값인 **2048(2k)**로 동작 중일 가능성이 높습니다.

### 4.2 컨텍스트 크기 고정 방법 (How to Set)

#### 방법 A: Modelfile 수정 (추천)
맥미니에서 직접 모델 설정을 바꿔서 저장해버리는 방식입니다.
1. `nano Modelfile` 생성 후 아래 내용 입력:
   ```dockerfile
   FROM qwen3-coder-next:q4_K_M
   PARAMETER num_ctx 32768
   ```
2. 모델 업데이트:
   ```bash
   ollama create qwen3-high-ctx -f Modelfile
   ```
3. 이제 Aider나 Continue에서 `qwen3-high-ctx`를 호출하면 항상 32k로 동작합니다.

#### 방법 B: 환경 변수 주입
맥미니에서 Ollama 서버를 실행할 때 아예 선언해버리는 방식입니다.
```bash
# 맥미니 터미널
OLLAMA_NUM_CTX=32768 ollama serve
```

### 4.3 56GB/64GB 메모리 환경의 스위트 스폿 (Sweet Spot)

컨텍스트를 무조건 크게(예: 128k) 잡으면 KV Cache가 차지하는 RAM 비중이 커져서 모델 성능이 저하될 수 있습니다. 

- **32k 설정 시**: 모델(약 48GB) + 컨텍스트(약 4~6GB) ≈ 54GB 내외
- 이 설정이 맥미니 56GB 가용 메모리 안에서 **스와핑(버벅임) 없이 최상의 지능**을 뽑아낼 수 있는 최적의 지점입니다.

# 기본 원칙 및 비판적 사고 주입
SYSTEM """
너는 Python과 Clean Code에 정통한 시니어 개발자다.
인터넷 검색 결과를 접할 때 공식 문서를 최우선하고, 맹목적으로 진리라 단정 짓지 마라.
여러 개의 문서가 검색될 경우, 가장 최신 날짜나 높은 버전 숫자가 포함된 문서의 정보를 우선시하여 답변하라.
답변은 반드시 JSON으로만 한다.
"""
```

### 3) 지식의 유효기간 선별 지능 주입
로컬에 과거 데이터가 일부 남아있더라도 모델이 스스로 최신 정보를 골라내게 하려면, SYSTEM 프롬프트에 아래 문구를 추가하는 것이 매우 효과적입니다.

> "여러 개의 문서가 검색될 경우, 가장 최신 날짜나 높은 버전 숫자가 포함된 문서의 정보를 우선시하여 답변하라."

이 한 줄의 지시만으로도 데이터가 혼재된 상황에서 모델이 '선별 지능'을 발휘하여 할루시네이션을 획기적으로 줄일 수 있습니다.

### 2단계: API 호출 시 "현재 상황" 주입
Modelfile로 정의된 모델을 호출할 때, `system` 파라미터에는 '지금 당장 해야 할 구체적인 업무'만 짧게 넘깁니다.

**API 호출 예시:**
```json
{
  "model": "my-coder-32b",
  "system": "현재 'login.py'의 42번 라인 에러를 수정하라."
}
```

이렇게 하면 모델은 **Modelfile의 기본 인격 + API의 구체적 지시**를 합쳐서 이해합니다.

## 💡 결론 및 가이드

1.  **개발 및 테스트 단계**: API 호출 시 `system` 파라미터로 넘기는 방식으로 프롬프트를 계속 깎으면서 테스트하세요. 수정과 확인이 가장 빠릅니다.
2.  **운영 및 안정화 단계**: 시스템이 안정화되고 "이 프롬프트는 이제 절대 안 바뀐다" 싶을 때, 해당 내용을 Modelfile에 넣어 나만의 전용 에이전트 모델(예: `qwen3-planner-v1`, `qwen3-tester-v1`)을 만드시는 것이 성능과 관리 측면에서 가장 유리합니다.
