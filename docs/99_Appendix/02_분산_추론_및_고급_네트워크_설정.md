# 02. 분산 추론 및 고급 네트워크 설정

단일 기기의 한계를 넘어 맥북과 맥미니를 썬더볼트로 연결하여 자원을 최적화하는 고급 설정 가이드입니다.

## 1. 썬더볼트 네트워킹 (Thunderbolt Bridge)

두 대의 Mac을 썬더볼트 케이블로 연결하여 초고속(최대 40Gbps) 전용 네트워크를 구축했습니다.

### ① 현재 구성 정보
- **맥북 (Gateway Node)**: `169.254.196.243` (자동 할당 IP)
- **맥미니 (Worker Node)**: `169.254.19.104` (자동 할당 IP)
- **전송 속도**: 썬더볼트 4/5 대역폭 활용 (매우 낮은 지연 시간)

### ② Ollama 원격 접속 설정 (맥미니 측)
맥북에서 맥미니의 Ollama에 접속할 수 있도록 맥미니에서 다음 설정을 수행해야 합니다.
```bash
# 맥미니 터미널에서 실행
launchctl setenv OLLAMA_HOST "0.0.0.0"
# 이후 Ollama 앱 재시작 필수
```

## 2. 분산 추론 전략 (Distributed Logic)

본 프로젝트는 모델의 레이어를 나누는 클러스터링(Exo 등) 대신, **역할에 따른 모델 분산 배치** 전략을 사용합니다.

- **이유**: 클러스터링은 메모리 합산은 가능하나 네트워크 오버헤드로 인해 토큰 생성 속도가 저하될 수 있습니다.
- **전략**: 
    - **MacBook**: 지능형 라우팅(`llama3.1:8b`) 및 가벼운 보안 검사 수행.
    - **Mac Mini**: 거대 모델(`qwen3-coder-next`)을 통한 고성능 추론 수행.
- **장점**: 각 기기가 독립적인 연산을 수행하므로 전체적인 시스템 처리량(Throughput)이 극대화됩니다.

## 3. 원격 모니터링 및 관리

맥북에서 맥미니의 상태를 실시간으로 확인하는 방법입니다.

### ① 라우터 헬스 체크
`router.py`에서 제공하는 `/health` 엔드포인트를 통해 두 노드의 연결 상태를 한눈에 파악할 수 있습니다.
```bash
curl http://localhost:8000/health
```

### ② GPU 메모리 최적화
각 기기에서 시스템이 LLM에 할당할 수 있는 최대 GPU 메모리를 조정합니다.
```bash
# 맥북 (24GB) 예시: 약 18GB 할당
sudo sysctl iogpu.wired_limit_mb=18432

# 맥미니 (64GB) 예시: 약 56GB 할당
sudo sysctl iogpu.wired_limit_mb=57344
```

## 4. Tailscale 기반 원격 접속 (권장)

**같은 공간**에서는 썬더볼트 브리지가 가장 빠르지만, **외출 중·휴대폰·다른 네트워크**에서 맥미니 Ollama에 접속하려면 **Tailscale**이 검증된 방법입니다.

- **구성**: 맥미니(서버), 맥북, 휴대폰을 Tailscale 한 계정으로 연결하면, Tailnet 전용 IP(`100.xx.xx.xx`)로 어디서든 맥미니에 접속 가능합니다.
- **맥미니 Ollama 실행**: 외부 접속을 허용하려면 `OLLAMA_ORIGINS="*" OLLAMA_HOST=0.0.0.0:11434 ollama serve` 로 실행해야 합니다.
- **상세 설정·기기별 앱·Cursor 대안**: [01_Setup 07. Tailscale 기반 Ollama 원격 접속](../01_Setup/07_Tailscale_기반_Ollama_원격_접속.md)을 참고하세요.

## 5. 트러블슈팅 (네트워크 연결 실패 시)

### 썬더볼트 브리지

1.  **아이피 확인**: `ifconfig` 명령어로 `bridge0` 인터페이스의 IP가 `router.py`에 설정된 값과 일치하는지 확인합니다.
2.  **핑 테스트**: `ping 169.254.19.104`를 통해 맥북에서 맥미니로의 물리적 연결을 확인합니다.
3.  **포트 개방**: 맥미니의 방화벽 설정에서 `11434` 포트가 차단되어 있는지 확인합니다.

### Tailscale

- Tailscale 앱이 맥미니·클라이언트 모두에서 **연결됨(Connected)** 상태인지 확인합니다.
- Ollama가 `OLLAMA_HOST=0.0.0.0`으로 수신 중인지, 그리고 `OLLAMA_ORIGINS="*"`로 설정되어 있는지 확인합니다.
