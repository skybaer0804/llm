# 06. Qwen3 토큰 사용량 및 효율성 가이드

Qwen3 모델은 다국어 대응력이 뛰어난 BPE(Byte Pair Encoding) 방식의 토커나이저를 사용하며, 약 15만 개 이상의 어휘(Vocabulary)를 보유하고 있어 한국어와 한자 효율이 매우 뛰어납니다. 본 가이드는 효율적인 컨텍스트 관리를 위한 언어별 토큰 사용량 기준을 제시합니다.

## 1. 언어별 토큰 효율 (Rule of Thumb)

Qwen3 모델의 일반적인 토큰당 글자 수 기준입니다. 언어별로 효율 차이가 확연하므로 프롬프트 작성 시 참고하시기 바랍니다.

| 언어 | 1개 토큰당 평균 글자 수 | 1,000자당 예상 토큰 수 | 특징 |
| :--- | :--- | :--- | :--- |
| **영어 (English)** | 약 3.5 ~ 4자 | 약 250 ~ 300개 | 단어 단위로 묶여 매우 효율적임 |
| **한글 (Korean)** | 약 1.2 ~ 1.8자 | 약 600 ~ 800개 | 조사와 어미가 잘 분리되어 효율이 준수함 |
| **한자 (Chinese/Hanja)** | 약 1.5 ~ 1.8자 | 약 550 ~ 700개 | 한자 한 글자가 1~2토큰으로 처리됨 |

## 2. 구체적인 예시 비교

동일한 의미의 문장을 처리할 때 소모되는 대략적인 토큰 수 비교입니다.
(예시 문장: "안녕하세요, 만나서 반갑습니다." / 한글 13자, 공백 포함)

- **한국어**: `안녕하세요,` (2~3) + `만나서` (1~2) + `반갑습니다.` (2~3) ≈ **총 5~8 토큰**
- **영어** ("Hello, nice to meet you."): `Hello,` (1) + `nice` (1) + `to` (1) + `meet` (1) + `you.` (1) ≈ **총 5~6 토큰**
- **한문/중문** ("你好，很高兴见到你。"): `你好` (1) + `，` (1) + `很高兴` (1) + `见到` (1) + `你` (1) + `。` (1) ≈ **총 6~7 토큰**

## 3. Qwen3만의 주요 특징

### ① "Thinking Mode"와 추론 토큰 (Reasoning Tokens)
Qwen3 모델(특히 **Thinking 버전**)을 사용할 때 가장 주의해야 할 점입니다.
- **추론 토큰**: 모델이 문제를 풀 때 내부적으로 거치는 '생각의 과정(Chain of Thought)'에서 수천 개의 토큰을 추가로 소모할 수 있습니다.
- **비용 관리**: 겉으로 보이는 답변은 짧더라도, 내부 추론 과정에서 발생하는 토큰으로 인해 실제 비용이나 컨텍스트 점유율이 예상보다 훨씬 클 수 있습니다.

### ② 압도적인 한자 압축률
Qwen 시리즈는 한자 압축률이 업계 최고 수준입니다. 
- 고전 번역, 전문 용어, 한자가 섞인 기술 문서를 처리할 때 GPT-4 등 타 모델 대비 토큰 소모량이 적어 경제적입니다.

## 4. 운영 및 최적화 팁

1.  **언어 선택**: 영어가 가장 토큰 효율이 좋으므로, 시스템 내부 통신이나 복잡한 로직 지시는 영어로 작성하는 것이 컨텍스트 절약에 유리합니다.
2.  **한국어/한자 활용**: 한국어와 한자 효율 역시 타 모델 대비 최적화되어 있으므로, 사용자 응답이나 문서화 작업 시 무리하게 영어를 고집할 필요는 없습니다.
3.  **컨텍스트 모니터링**: Thinking Mode 사용 시 `num_ctx` 설정을 평소보다 넉넉하게(최소 32k 이상) 잡아 추론 과정에서 컨텍스트가 잘리는 것을 방지하십시오.
