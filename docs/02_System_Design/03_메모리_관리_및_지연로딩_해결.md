# 03. 메모리 관리 및 지연로딩 해결

M4 Pro 64GB 통합 메모리 환경에서 대형 모델을 안정적으로 운영하고, 모델 스왑으로 인한 지연 시간(Swap Delay)을 최소화하여 실시간 협업 경험을 제공하기 위한 최적화 전략을 다룹니다.

## 1. 지연 로딩(Lazy Loading)의 이해와 문제점
Ollama는 기본적으로 요청이 올 때 모델을 메모리에 로드합니다. `qwen3-coder-next:q4_K_M`와 같은 대형 모델(52GB)은 로드하는 데만 수십 초가 소요될 
수 있으며, 이는 AutoGen 에이전트 간의 대화 흐름을 끊는 '지연 로딩 문제'를 야기합니다.

## 1. macOS 통합 메모리 제한 해제 (VRAM 할당량 확대)

macOS는 기본적으로 시스템 안정성을 위해 통합 메모리의 약 66%~75%만 GPU(VRAM)용으로 할당합니다. 64GB 모델의 경우 약 42~48GB가 한계이지만, 대형 모델(52GB 등)을 운용하기 위해 이를 **85% 수준(약 54GB)**까지 상향 조정합니다.

### 설정 방법 (터미널)
다음 명령어를 실행하고 시스템을 재부팅합니다. (5~8GB 정도는 OS용으로 남겨두는 것이 안전합니다.)
```bash
sudo sysctl iogpu.UnifiedMemoryLimitBytes=57982058496
```

---

## 2. Smart Swap: 에이전트 운용 전략

모델 로딩 지연을 극복하기 위해 모든 모델을 수시로 스왑하는 대신, 호출 빈도와 모델 크기에 따라 상주 여부를 결정하는 전략을 사용합니다.

### 1) TDD 루프 상주 (Coding & Testing Phase)
가장 빈번하게 대화가 오가는 **Coder**와 **Reviewer/Tester** 모델을 메모리에 상주시켜 지연 시간 0초의 즉각적인 피드백 루프를 형성합니다.

- **Coder (32b)**: 약 19GB
- **Reviewer/Tester (14b)**: 약 9GB
- **합계**: 약 28GB (64GB 환경에서 상주 가능)
- **설정**: `keep_alive`를 `2h` 정도로 넉넉하게 설정하여 루프 동안 언로드를 방지합니다.

### 2) 설계 단계 전환 (Architect Phase)
설계자가 투입될 때는 기존 모델들을 언로드하고 대형 모델에게 모든 자원을 할당합니다.

- **Architect (52GB)**: 로드 시 Coder/Reviewer를 즉시 언로드(`keep_alive: 0`).
- **효과**: 54GB의 가용 VRAM을 Architect가 독점하여 추론 속도와 컨텍스트 유지력을 극대화합니다.

---

## 3. 지연 시간 극복을 위한 4가지 추가 전략

1.  **전담 테스터 모델 고정 (VRAM Residency)**: `keep_alive: -1`을 사용하여 테스터 모델을 영구 유지.
2.  **가벼운 '중계자(Router)' 도입**: qwen3:1.7b 모델을 상주시켜 단순 에러 수정이나 단계 판단(설계/코딩/수정)을 즉시 처리.
3.  **VRAM 계층화 (Quantization 최적화)**:
    *   Planner(Architect): 정교함이 필수이므로 `q4_K_M` 유지.
    *   Coder: 문법 규칙 중심이므로 `q3_K_S` 또는 `q2_K`로 다이어트(19GB → 12GB)하여 로딩 속도 2배 향상.
4.  **Ollama 병렬 로딩**: `OLLAMA_MAX_LOADED_MODELS` 옵션을 조정하여 Tester와 Coder가 동시에 상주하도록 설정.

---

## 4. 실전 최적화 코드 예시 (Python)

에이전트 호출 시 `keep_alive` 파라미터를 동적으로 제어하여 메모리를 효율적으로 관리합니다.

```python
import ollama

# 1. 아키텍트 호출: 설계 직후 즉시 메모리 반납 (공간 확보)
ollama.generate(model='qwen3-coder-next:q4_K_M', prompt='...', keep_alive='0')

# 2. 테스터 호출: 빠른 루프를 위해 30분 동안 메모리 유지
ollama.generate(model='qwen3-coder:14b', prompt='...', keep_alive='30m')

# 3. 코더 호출: 지속적인 수정을 위해 상주 유지
ollama.generate(model='qwen3-coder:32b', prompt='...', keep_alive='2h')
```
