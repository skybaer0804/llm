# 01. 로컬 LLM 개요 및 구축 목적

본 문서는 Mac M4 Pro 환경에서 로컬 LLM(Large Language Model)을 구축하고, AutoGen 프레임워크를 활용하여 멀티 에이전트 개발 협업 시스템을 운영하기 위한 가이드입니다.

## 1. 로컬 LLM의 정의 및 장점

로컬 LLM은 OpenAI의 ChatGPT나 Google의 Gemini와 같은 클라우드 기반 프론티어 LLM과 달리, 개인 PC나 사내 서버에 직접 모델을 다운로드하여 구동하는 방식입니다.

- **데이터 보안 및 프라이버시**: 모든 데이터 처리가 로컬 기기 내에서 이루어져 민감 정보 유출 위험이 없습니다.
- **비용 절감**: 초기 하드웨어 투자 이후 API 사용료 없이 무제한으로 모델을 활용할 수 있습니다.
- **오프라인 환경 활용**: 인터넷 연결 없이도 모델을 사용할 수 있습니다.
- **커스터마이징**: 특정 도메인에 맞게 모델을 튜닝하거나 환경을 자유롭게 구성할 수 있습니다.

## 2. 주요 모델 트렌드 (2026년 기준)

현재 로컬 LLM 시장은 효율성과 성능을 모두 잡은 오픈 소스 모델들이 주도하고 있습니다.

- **Qwen (Alibaba)**: 특히 코딩 및 논리적 추론 성능이 뛰어나며, 다양한 파라미터 크기를 제공합니다.
- **DeepSeek**: 알고리즘 중심의 효율적인 구조로 높은 가성비와 성능을 보여줍니다.
- **Llama (Meta)**: 가장 대중적인 모델로 풍부한 생태계를 보유하고 있습니다.
- **Gemma (Google)**: 구글의 최신 기술이 반영된 가볍고 강력한 모델입니다.

## 3. 최종 구축 목적

본 프로젝트의 최종 목적은 **Mac M4 Pro 64GB 환경에서 최신 Qwen3 모델 시리즈를 활용한 AutoGen 멀티 에이전트 협업 시스템 구축**입니다.

- **🛰️ Router (Gateway)**: `qwen2.5:7b` - 시스템 관문으로서 요청 분석 및 에이전트 라우팅 수행.
- **🧠 Planner (Architect)**: `qwen3-coder-next:q4_K_M` - 요구사항 분석 및 시스템 아키텍처 설계.
- **💻 Dev1_Senior (Coder)**: `qwen3-coder:32b` - 핵심 기능 구현 및 TDD 기반 고속 코드 생성.
- **👀 Dev2_Reviewer / Tester**: `qwen3-coder:14b` - 코드 품질 검수 및 pytest 기반 자동화 테스트 수행.
- **📝 Documenter**: `qwen2.5:7b` - 협업 기록을 취합한 기술 문서 및 최종 리포트 작성.
- **🚀 Frontier LLM**: (외부) GPT-4o / Claude - 로컬 한계 초과 시 활용하는 SOS 파트너.

이를 통해 기획부터 구현, 검증, 문서화까지 자동화된 'Full-Stack' 개발 파이프라인을 로컬 환경에서 완벽하게 구현하는 것을 목표로 합니다.
